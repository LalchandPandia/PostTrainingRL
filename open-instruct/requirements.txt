#torch
pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128
#install falsh-attn from a submitted slurm script
pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.0.post2/flash_attn-2.8.0.post2+cu12torch2.7cxx11abiFALSE-cp312-cp312-linux_x86_64.whl
#to install remaining packages
#remove falsh-attn installation in pyproject.toml
# remove this "flash-attn>=2.8.3; platform_system != 'Darwin'", from dependencies list
pip install -e .